---
title: "Understanding Neural Networks with Logarithm Determinant Entropy Estimator"
collection: publications
permalink: /publication/2021-05-08-LogDet-number-1
excerpt: 'We design a metric to estimating information entropy in neural networks.'
date: 2021-05-08
venue: 'arxiv'
paperurl: 'http://floatingCatty.github.io/files/2105.03705.pdf'
citation: 'Zhouyin, Zhanghao, and Ding Liu. "Understanding neural networks with logarithm determinant entropy estimator." arXiv preprint arXiv:2105.03705 (2021).'
---
Understanding the informative behaviour of deep neural networks is challenged by misused estimators and the complexity of network structure, which leads to inconsistent observations and diversified interpretation. Here we propose the LogDet estimator -- a reliable matrix-based entropy estimator that approximates Shannon differential entropy. We construct informative measurements based on LogDet estimator, verify our method with comparable experiments and utilize it to analyse neural network behaviour. Our results demonstrate the LogDet estimator overcomes the drawbacks that emerge from highly diverse and degenerated distribution thus is reliable to estimate entropy in neural networks. The Network analysis results also find a functional distinction between shallow and deeper layers, which can help understand the compression phenomenon in the Information bottleneck theory of neural networks.


[Download paper here](https://arxiv.org/pdf/2105.03705.pdf)

Recommended citation: Zhouyin, Zhanghao, and Ding Liu. "Understanding neural networks with logarithm determinant entropy estimator." arXiv preprint arXiv:2105.03705 (2021).
